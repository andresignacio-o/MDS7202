{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDZmp2BO9KnQ"
      },
      "source": [
        "# **Laboratorio 8: Ready, Set, Deploy! üë©‚ÄçüöÄüë®‚ÄçüöÄ**\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos - Primavera 2025</strong></center>\n",
        "\n",
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Diego Cortez, Gabriel Iturra\n",
        "- Auxiliares: Melanie Pe√±a, Valentina Rojas\n",
        "- Ayudantes: Nicol√°s Cabello, Cristopher Urbina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdGqUgwX9pGQ"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1:Andres O√±ate\n",
        "- Nombre de alumno 2:Javier Zapata\n",
        "\n",
        "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/andresignacio-o/MDS7202)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YraSOKrf9yMl"
      },
      "source": [
        "## Temas a tratar\n",
        "\n",
        "- Entrenamiento y registro de modelos usando MLFlow.\n",
        "- Despliegue de modelo usando FastAPI\n",
        "- Containerizaci√≥n del proyecto usando Docker\n",
        "\n",
        "## Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: Entregas Martes a las 23:59.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria, pero se recomienda **fuertemente** asistir.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquier material del curso que estimen conveniente.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "\n",
        "- Generar una soluci√≥n a un problema a partir de ML\n",
        "- Desplegar su soluci√≥n usando MLFlow, FastAPI y Docker\n",
        "\n",
        "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D98okEzUE8hb"
      },
      "source": [
        "# **Introducci√≥n**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSiuBfGiFlQM"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExODJnMHJzNzlkNmQweXoyY3ltbnZ2ZDlxY2c0aW5jcHNzeDNtOXBsdCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/AbPdhwsMgjMjax5reo/giphy.gif\" width=\"400\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPn8R-6u877j"
      },
      "source": [
        "\n",
        "\n",
        "Consumida en la tristeza el despido de Renac√≠n, Smapina ha deca√≠do en su desempe√±o, lo que se ha traducido en un irregular tratamiento del agua. Esto ha implicado una baja en la calidad del agua, llegando a haber algunos puntos de la comuna en la que el vital elemento no es apto para el consumo humano. Es por esto que la sanitaria p√∫blica de la municipalidad de Maip√∫ se ha contactado con ustedes para que le entreguen una urgente soluci√≥n a este problema (a la vez que dejan a Smapina, al igual que Renac√≠n, sin trabajo üòî).\n",
        "\n",
        "El problema que la empresa le ha solicitado resolver es el de elaborar un sistema que les permita saber si el agua es potable o no. Para esto, la sanitaria les ha proveido una base de datos con la lectura de m√∫ltiples sensores IOT colocados en diversas ca√±er√≠as, conductos y estanques. Estos sensores se√±alan nueve tipos de mediciones qu√≠micas y m√°s una etiqueta elaborada en laboratorio que indica si el agua es potable o no el agua.\n",
        "\n",
        "La idea final es que puedan, en el caso que el agua no sea potable, dar un aviso inmediato para corregir el problema. Tenga en cuenta que parte del equipo docente vive en Maip√∫ y su intoxicaci√≥n podr√≠a implicar graves problemas para el cierre del curso.\n",
        "\n",
        "Atributos:\n",
        "\n",
        "1. pH value\n",
        "2. Hardness\n",
        "3. Solids (Total dissolved solids - TDS)\n",
        "4. Chloramines\n",
        "5. Sulfate\n",
        "6. Conductivity\n",
        "7. Organic_carbon\n",
        "8. Trihalomethanes\n",
        "9. Turbidity\n",
        "\n",
        "Variable a predecir:\n",
        "\n",
        "10. Potability (1 si es potable, 0 no potable)\n",
        "\n",
        "Descripci√≥n de cada atributo se pueden encontrar en el siguiente link: [dataset](https://www.kaggle.com/adityakadiwal/water-potability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aIr6KegWsjS"
      },
      "source": [
        "# **1. Optimizaci√≥n de modelos con Optuna + MLFlow (2.0 puntos)**\n",
        "\n",
        "El objetivo de esta secci√≥n es que ustedes puedan combinar Optuna con MLFlow para poder realizar la optimizaci√≥n de los hiperpar√°metros de sus modelos.\n",
        "\n",
        "Como a√∫n no hemos hablado nada sobre `MLFlow` cabe preguntarse: **¬°¬øQu√© !\"#@ es `MLflow`?!**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/eusgDKT4smQAAAAC/matthew-perry-chandler-bing.gif\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "## **MLFlow**\n",
        "\n",
        "`MLflow` es una plataforma de c√≥digo abierto que simplifica la gesti√≥n y seguimiento de proyectos de aprendizaje autom√°tico. Con sus herramientas, los desarrolladores pueden organizar, rastrear y comparar experimentos, adem√°s de registrar modelos y controlar versiones.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://spark.apache.org/images/mlflow-logo.png\" width=\"350\">\n",
        "</p>\n",
        "\n",
        "Si bien esta plataforma cuenta con un gran n√∫mero de herramientas y funcionalidades, en este laboratorio trabajaremos con dos:\n",
        "1. **Runs**: Registro que constituye la informaci√≥n guardada tras la ejecuci√≥n de un entrenamiento. Cada `run` tiene su propio run_id, el cual sirve como identificador para el entrenamiento en s√≠ mismo. Dentro de cada `run` podremos acceder a informaci√≥n como los hiperpar√°metros utilizados, las m√©tricas obtenidas, las librer√≠as requeridas y hasta nos permite descargar el modelo entrenado.\n",
        "2. **Experiments**: Se utilizan para agrupar y organizar diferentes ejecuciones de modelos (`runs`). En ese sentido, un experimento puede agrupar 1 o m√°s `runs`. De esta manera, es posible tambi√©n registrar m√©tricas, par√°metros y archivos (artefactos) asociados a cada experimento.\n",
        "\n",
        "### **Todo bien pero entonces, ¬øc√≥mo se usa en la pr√°ctica `MLflow`?**\n",
        "\n",
        "Es sencillo! Considerando un problema de machine learning gen√©rico, podemos registrar la informaci√≥n relevante del entrenamiento ejecutando `mlflow.autolog()` antes entrenar nuestro modelo. Veamos este bonito ejemplo facilitado por los mismos creadores de `MLflow`:\n",
        "\n",
        "```python\n",
        "#!pip install mlflow\n",
        "import mlflow # importar mlflow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "db = load_diabetes()\n",
        "X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n",
        "\n",
        "# Create and train models.\n",
        "rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n",
        "\n",
        "mlflow.autolog() # registrar autom√°ticamente informaci√≥n del entrenamiento\n",
        "with mlflow.start_run(): #¬†delimita inicio y fin del run\n",
        "    #¬†aqu√≠ comienza el run\n",
        "    rf.fit(X_train, y_train) # train the model\n",
        "    predictions = rf.predict(X_test) # Use the model to make predictions on the test dataset.\n",
        "    # aqu√≠ termina el run\n",
        "```\n",
        "\n",
        "Si ustedes ejecutan el c√≥digo anterior en sus m√°quinas locales (desde un jupyter notebook por ejemplo) se dar√°n cuenta que en su directorio *root* se ha creado la carpeta `mlruns`. Esta carpeta lleva el tracking de todos los entrenamientos ejecutados desde el directorio root (importante: si se cambian de directorio y vuelven a ejecutar el c√≥digo anterior, se crear√° otra carpeta y no tendr√°n acceso al entrenamiento anterior). Para visualizar estos entrenamientos, `MLflow` nos facilita hermosa interfaz visual a la que podemos acceder ejecutando:\n",
        "\n",
        "```\n",
        "mlflow ui\n",
        "```\n",
        "\n",
        "y luego pinchando en la ruta http://127.0.0.1:5000 que nos retorna la terminal. Veamos en vivo algunas de sus funcionalidades!\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExZXVuM3A5MW1heDFpa21qbGlwN2pyc2VoNnZsMmRzODZxdnluemo2bCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3o84sq21TxDH6PyYms/giphy.gif\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Les dejamos tambi√©n algunos comandos √∫tiles:\n",
        "\n",
        "- `mlflow.create_experiment(\"nombre_experimento\")`: Les permite crear un nuevo experimento para agrupar entrenamientos\n",
        "- `mlflow.log_metric(\"nombre_m√©trica\", m√©trica)`: Les permite registrar una m√©trica *custom* bajo el nombre de \"nombre_m√©trica\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptP_ygr7S04t"
      },
      "source": [
        "## **1.1 Combinando Optuna + MLflow (2.0 puntos)**\n",
        "\n",
        "Ahora que tenemos conocimiento de ambas herramientas, intentemos ahora combinarlas para **m√°s sabor**. El objetivo de este apartado es simple: automatizar la optimizaci√≥n de los par√°metros de nuestros modelos usando `Optuna` y registrando de forma autom√°tica cada resultado en `MLFlow`.\n",
        "\n",
        "Considerando el objetivo planteado, se le pide completar la funci√≥n `optimize_model`, la cual debe:\n",
        "- **Optimizar los hiperpar√°metros del modelo `XGBoost` usando `Optuna`.**\n",
        "- **Registrar cada entrenamiento en un experimento nuevo**, asegur√°ndose de que la m√©trica `f1-score` se registre como `\"valid_f1\"`. No se deben guardar todos los experimentos en *Default*; en su lugar, cada `experiment` y `run` deben tener nombres interpretables, reconocibles y diferentes a los nombres por defecto (por ejemplo, para un run: \"XGBoost con lr 0.1\").\n",
        "- **Guardar los gr√°ficos de Optuna** dentro de una carpeta de artefactos de Mlflow llamada `/plots`.\n",
        "- **Devolver el mejor modelo** usando la funci√≥n `get_best_model` y serializarlo en el disco con `pickle.dump`. Luego, guardar el modelo en la carpeta `/models`.\n",
        "- **Guardar el c√≥digo en `optimize.py`**. La ejecuci√≥n de `python optimize.py` deber√≠a ejecutar la funci√≥n `optimize_model`.\n",
        "- **Guardar las versiones de las librer√≠as utilizadas** en el desarrollo.\n",
        "- **Respalde las configuraciones del modelo final y la importancia de las variables** en un gr√°fico dentro de la carpeta `/plots` creada anteriormente.\n",
        "\n",
        "*Hint: Le puede ser √∫til revisar los par√°metros que recibe `mlflow.start_run`*\n",
        "\n",
        "```python\n",
        "def get_best_model(experiment_id):\n",
        "    runs = mlflow.search_runs(experiment_id)\n",
        "    best_model_id = runs.sort_values(\"metrics.valid_f1\")[\"run_id\"].iloc[0]\n",
        "    best_model = mlflow.sklearn.load_model(\"runs:/\" + best_model_id + \"/model\")\n",
        "\n",
        "    return best_model\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, tempfile, pickle\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import optuna\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from optuna.visualization.matplotlib import plot_optimization_history, plot_param_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as XGBClassifier # Ya lo tienes\n",
        "from xgboost import DMatrix # A√±adir esta\n",
        "from xgboost.callback import EarlyStopping # Necesaria para callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_best_model(experiment_id):\n",
        "    runs = mlflow.search_runs(experiment_id)\n",
        "    best_model_id = runs.sort_values(\"metrics.valid_f1\")[\"run_id\"].iloc[0]\n",
        "    best_model = mlflow.sklearn.load_model(\"runs:/\" + best_model_id + \"/model\")\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _log_versions():\n",
        "    import mlflow as _mlf, optuna as _opt, xgboost as _xgb, sklearn as _sk, numpy as _np\n",
        "    versions = {\n",
        "        \"python\": f\"{os.sys.version_info.major}.{os.sys.version_info.minor}.{os.sys.version_info.micro}\",\n",
        "        \"mlflow\": _mlf.__version__, \"optuna\": _opt.__version__,\n",
        "        \"xgboost\": _xgb.__version__, \"scikit_learn\": _sk.__version__, \"numpy\": _np.__version__,\n",
        "    }\n",
        "    with tempfile.TemporaryDirectory() as td:\n",
        "        fp = os.path.join(td, \"versions.json\")\n",
        "        with open(fp, \"w\", encoding=\"utf-8\") as f: json.dump(versions, f, indent=2)\n",
        "        mlflow.log_artifact(fp)  # ra√≠z del run\n",
        "\n",
        "\n",
        "def _save_matplotlib(fig, name, artifact_path=\"plots\"):\n",
        "    with tempfile.TemporaryDirectory() as td:\n",
        "        out = os.path.join(td, name)\n",
        "        fig.savefig(out, bbox_inches=\"tight\")\n",
        "        mlflow.log_artifact(out, artifact_path=artifact_path)\n",
        "    plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTNLPUnm8yzD"
      },
      "outputs": [],
      "source": [
        "def optimize_model(X_train, y_train, X_valid, y_valid, experiment_name=None, n_trials=20, random_state=42):\n",
        "    print(\"[INFO] Iniciando optimize_model()\")\n",
        "    avg = \"binary\" if len(np.unique(y_valid)) == 2 else \"macro\"\n",
        "    print(f\"[INFO] f1 average = {avg}\")\n",
        "\n",
        "    if experiment_name is None:\n",
        "        experiment_name = f\"XGB_Optuna_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    print(f\"[INFO] Usando experimento: {experiment_name}\")\n",
        "\n",
        "    exp = mlflow.get_experiment_by_name(experiment_name)\n",
        "    experiment_id = exp.experiment_id if exp else mlflow.create_experiment(experiment_name)\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    def objective(trial: optuna.Trial) -> float:\n",
        "        params = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 600),\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "            \"random_state\": random_state,\n",
        "            \"n_jobs\": -1,\n",
        "            \"objective\": \"binary:logistic\" if avg == \"binary\" else \"multi:softprob\",\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"eval_metric\": \"logloss\",\n",
        "        }\n",
        "        run_name = f\"XGBoost con lr {params['learning_rate']:.3f}\"\n",
        "        print(f\"[TRIAL {trial.number}] {run_name}\")\n",
        "\n",
        "        with mlflow.start_run(run_name=run_name, experiment_id=experiment_id):\n",
        "            _log_versions()  # √∫til si el run falla en tu infra\n",
        "            mlflow.log_params(params)\n",
        "\n",
        "\n",
        "            early_stop_callback = EarlyStopping(\n",
        "                rounds=30,  # Same as your original early_stopping_rounds value\n",
        "                min_delta=1e-5,\n",
        "                save_best=True,\n",
        "                # Use maximize=False because 'logloss' (your eval_metric) is minimized\n",
        "                maximize=False \n",
        "            )\n",
        "\n",
        "            model = XGBClassifier(**params)\n",
        "            model.set_params(callbacks=[XGBClassifier.callback.EarlyStopping(rounds=10)])\n",
        "            model.fit(\n",
        "                X_train, \n",
        "                y_train, \n",
        "                eval_set=[(X_valid, y_valid)], \n",
        "                verbose=False, \n",
        "            )\n",
        "\n",
        "            if avg == \"binary\":\n",
        "                y_prob = model.predict_proba(X_valid)[:, 1]\n",
        "                y_pred = (y_prob >= 0.5).astype(int)\n",
        "            else:\n",
        "                y_pred = np.argmax(model.predict_proba(X_valid), axis=1)\n",
        "\n",
        "            f1 = f1_score(y_valid, y_pred, average=avg)\n",
        "            print(f\"[TRIAL {trial.number}] valid_f1={f1:.5f}\")\n",
        "            mlflow.log_metric(\"valid_f1\", f1)\n",
        "\n",
        "            # registrar el modelo del trial\n",
        "            mlflow.sklearn.log_model(model, artifact_path=\"model\")\n",
        "            return f1\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
        "\n",
        "    print(f\"[INFO] Mejores params: {study.best_params} | best f1={study.best_value:.5f}\")\n",
        "\n",
        "    # Gr√°ficos de Optuna -> /plots\n",
        "    fig1 = plot_optimization_history(study); _save_matplotlib(fig1.figure, \"optuna_optimization_history.png\")\n",
        "    fig2 = plot_param_importances(study);   _save_matplotlib(fig2.figure, \"optuna_param_importances.png\")\n",
        "\n",
        "    # Cargar mejor modelo con la funci√≥n proporcionada (sin modificar)\n",
        "    print(\"[INFO] Cargando mejor modelo con get_best_model()‚Ä¶\")\n",
        "    best_model = get_best_model(experiment_id)\n",
        "\n",
        "    # Guardar config final y feature importance -> /plots\n",
        "    cfg = getattr(best_model, \"get_xgb_params\", None)\n",
        "    best_cfg = cfg() if cfg else best_model.get_params()\n",
        "    with tempfile.TemporaryDirectory() as td:\n",
        "        cfgp = os.path.join(td, \"final_model_config.json\")\n",
        "        with open(cfgp, \"w\", encoding=\"utf-8\") as f: json.dump(best_cfg, f, indent=2, default=str)\n",
        "        mlflow.log_artifact(cfgp, artifact_path=\"plots\")\n",
        "\n",
        "    try:\n",
        "        importances = getattr(best_model, \"feature_importances_\", None)\n",
        "        if importances is not None:\n",
        "            order = np.argsort(importances)[::-1][:20]\n",
        "            fig, ax = plt.subplots(figsize=(8, 5))\n",
        "            ax.bar(range(len(order)), importances[order])\n",
        "            ax.set_xticks(range(len(order)))\n",
        "            ax.set_xticklabels([f\"f{i}\" for i in order], rotation=45, ha=\"right\")\n",
        "            ax.set_title(\"XGBoost - Importancia de variables\")\n",
        "            _save_matplotlib(fig, \"final_feature_importances.png\", artifact_path=\"plots\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] No se pudo graficar importancias: {e}\")\n",
        "\n",
        "    # Serializar mejor modelo en /models\n",
        "    with mlflow.start_run(run_name=\"Exportar mejor modelo\", experiment_id=experiment_id):\n",
        "        with tempfile.TemporaryDirectory() as td:\n",
        "            mp = os.path.join(td, \"best_model.pkl\")\n",
        "            with open(mp, \"wb\") as f: pickle.dump(best_model, f)\n",
        "            mlflow.log_artifact(mp, artifact_path=\"models\")\n",
        "        print(\"[INFO] Modelo serializado a /models/best_model.pkl\")\n",
        "\n",
        "    print(\"[OK] optimize_model terminado.\")\n",
        "    return experiment_id, study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La versi√≥n de XGBoost en este entorno es: 3.0.5\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "print(f\"La versi√≥n de XGBoost en este entorno es: {xgb.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on function fit in module xgboost.sklearn:\n",
            "\n",
            "fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[Sequence[Tuple[Any, Any]]] = None, verbose: Union[bool, int, NoneType] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Optional[Sequence[Any]] = None, base_margin_eval_set: Optional[Sequence[Any]] = None, feature_weights: Optional[Any] = None) -> 'XGBClassifier'\n",
            "    Fit gradient boosting classifier.\n",
            "    \n",
            "    Note that calling ``fit()`` multiple times will cause the model object to be\n",
            "    re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
            "    pass ``xgb_model`` argument.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    X :\n",
            "        Input feature matrix. See :ref:`py-data` for a list of supported types.\n",
            "    \n",
            "        When the ``tree_method`` is set to ``hist``, internally, the\n",
            "        :py:class:`QuantileDMatrix` will be used instead of the :py:class:`DMatrix`\n",
            "        for conserving memory. However, this has performance implications when the\n",
            "        device of input data is not matched with algorithm. For instance, if the\n",
            "        input is a numpy array on CPU but ``cuda`` is used for training, then the\n",
            "        data is first processed on CPU then transferred to GPU.\n",
            "    y :\n",
            "        Labels\n",
            "    sample_weight :\n",
            "        instance weights\n",
            "    base_margin :\n",
            "        Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "    eval_set :\n",
            "        A list of (X, y) tuple pairs to use as validation sets, for which\n",
            "        metrics will be computed.\n",
            "        Validation metrics will help us track the performance of the model.\n",
            "    \n",
            "    verbose :\n",
            "        If `verbose` is True and an evaluation set is used, the evaluation metric\n",
            "        measured on the validation set is printed to stdout at each boosting stage.\n",
            "        If `verbose` is an integer, the evaluation metric is printed at each\n",
            "        `verbose` boosting stage. The last boosting stage / the boosting stage found\n",
            "        by using `early_stopping_rounds` is also printed.\n",
            "    xgb_model :\n",
            "        file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
            "        loaded before training (allows training continuation).\n",
            "    sample_weight_eval_set :\n",
            "        A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
            "        object storing instance weights for the i-th validation set.\n",
            "    base_margin_eval_set :\n",
            "        A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
            "        object storing base margin for the i-th validation set.\n",
            "    feature_weights :\n",
            "    \n",
            "        .. deprecated:: 3.0.0\n",
            "    \n",
            "        Use `feature_weights` in :py:meth:`__init__` or :py:meth:`set_params`\n",
            "        instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "help(XGBClassifier.fit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-20 21:05:36,920] A new study created in memory with name: no-name-42d192c5-762b-4cbc-a5a6-d39d81d05000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[MAIN] Generando datos de ejemplo (puedes reemplazar por tus splits reales)‚Ä¶\n",
            "[INFO] Iniciando optimize_model()\n",
            "[INFO] f1 average = binary\n",
            "[INFO] Usando experimento: XGB_Optuna_20251020_210536\n",
            "[TRIAL 0] XGBoost con lr 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:05:38 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 0] valid_f1=0.86275\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:05:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:05:55,895] Trial 0 finished with value: 0.8627450980392157 and parameters: {'n_estimators': 343, 'learning_rate': 0.005209676092258275, 'max_depth': 10, 'subsample': 0.9498245507338356, 'colsample_bytree': 0.7333764539101391}. Best is trial 0 with value: 0.8627450980392157.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 1] XGBoost con lr 0.025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:05:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 1] valid_f1=0.85149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:06:01,734] Trial 1 finished with value: 0.8514851485148515 and parameters: {'n_estimators': 164, 'learning_rate': 0.025066652521362327, 'max_depth': 7, 'subsample': 0.7363324970702004, 'colsample_bytree': 0.8404524189952163}. Best is trial 0 with value: 0.8627450980392157.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 2] XGBoost con lr 0.001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:02 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 2] valid_f1=0.84848\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:06:08,116] Trial 2 finished with value: 0.8484848484848485 and parameters: {'n_estimators': 573, 'learning_rate': 0.0013059572577421342, 'max_depth': 7, 'subsample': 0.6187343158275921, 'colsample_bytree': 0.988847063548256}. Best is trial 0 with value: 0.8627450980392157.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 3] XGBoost con lr 0.017\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:08 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 3] valid_f1=0.84314\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:06:14,314] Trial 3 finished with value: 0.8431372549019608 and parameters: {'n_estimators': 310, 'learning_rate': 0.016720177659788234, 'max_depth': 7, 'subsample': 0.7299336773684806, 'colsample_bytree': 0.8512691408692168}. Best is trial 0 with value: 0.8627450980392157.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 4] XGBoost con lr 0.018\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:14 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 4] valid_f1=0.86000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:06:20,145] Trial 4 finished with value: 0.86 and parameters: {'n_estimators': 483, 'learning_rate': 0.01846388808588802, 'max_depth': 8, 'subsample': 0.8002304302893364, 'colsample_bytree': 0.7583121306900291}. Best is trial 0 with value: 0.8627450980392157.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 5] XGBoost con lr 0.009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:21 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 5] valid_f1=0.86000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:06:26,791] Trial 5 finished with value: 0.86 and parameters: {'n_estimators': 536, 'learning_rate': 0.009052662595011852, 'max_depth': 10, 'subsample': 0.952956732995419, 'colsample_bytree': 0.6293119750710566}. Best is trial 0 with value: 0.8627450980392157.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 6] XGBoost con lr 0.022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 6] valid_f1=0.85149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:06:32,569] Trial 6 finished with value: 0.8514851485148515 and parameters: {'n_estimators': 146, 'learning_rate': 0.022259786809636056, 'max_depth': 5, 'subsample': 0.8023465889088147, 'colsample_bytree': 0.6903847810462415}. Best is trial 0 with value: 0.8627450980392157.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 7] XGBoost con lr 0.002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:33 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 7] valid_f1=0.86000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:38 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:06:38,923] Trial 7 finished with value: 0.86 and parameters: {'n_estimators': 446, 'learning_rate': 0.0023820907783907138, 'max_depth': 7, 'subsample': 0.9576830885720021, 'colsample_bytree': 0.6986076455966115}. Best is trial 0 with value: 0.8627450980392157.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 8] XGBoost con lr 0.008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:39 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 8] valid_f1=0.86000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:06:45,128] Trial 8 finished with value: 0.86 and parameters: {'n_estimators': 352, 'learning_rate': 0.008352102365875023, 'max_depth': 8, 'subsample': 0.8619123964626871, 'colsample_bytree': 0.7402856126332731}. Best is trial 0 with value: 0.8627450980392157.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 9] XGBoost con lr 0.034\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:45 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 9] valid_f1=0.85149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:06:51,283] Trial 9 finished with value: 0.8514851485148515 and parameters: {'n_estimators': 569, 'learning_rate': 0.03424395360201353, 'max_depth': 7, 'subsample': 0.9588208436264588, 'colsample_bytree': 0.6720129338042249}. Best is trial 0 with value: 0.8627450980392157.\n",
            "2025/10/20 21:06:51 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 10] XGBoost con lr 0.236\n",
            "[TRIAL 10] valid_f1=0.84314\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:56 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:06:56,842] Trial 10 finished with value: 0.8431372549019608 and parameters: {'n_estimators': 274, 'learning_rate': 0.23646098410763403, 'max_depth': 3, 'subsample': 0.8839445952049956, 'colsample_bytree': 0.9239339844541616}. Best is trial 0 with value: 0.8627450980392157.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 11] XGBoost con lr 0.095\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:06:57 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 11] valid_f1=0.90196\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:07:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:07:02,930] Trial 11 finished with value: 0.9019607843137255 and parameters: {'n_estimators': 422, 'learning_rate': 0.09478695353462088, 'max_depth': 10, 'subsample': 0.6209686514551447, 'colsample_bytree': 0.7651064381700929}. Best is trial 11 with value: 0.9019607843137255.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 12] XGBoost con lr 0.157\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:07:03 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 12] valid_f1=0.87379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:07:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:07:08,675] Trial 12 finished with value: 0.8737864077669902 and parameters: {'n_estimators': 398, 'learning_rate': 0.15695356809765026, 'max_depth': 10, 'subsample': 0.6111581799828488, 'colsample_bytree': 0.8069059324484886}. Best is trial 11 with value: 0.9019607843137255.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 13] XGBoost con lr 0.157\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:07:08 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 13] valid_f1=0.83168\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:07:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:07:14,810] Trial 13 finished with value: 0.8316831683168316 and parameters: {'n_estimators': 413, 'learning_rate': 0.157114535544959, 'max_depth': 9, 'subsample': 0.6002742749800771, 'colsample_bytree': 0.8163193208850086}. Best is trial 11 with value: 0.9019607843137255.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 14] XGBoost con lr 0.089\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:07:15 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRIAL 14] valid_f1=0.86000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/20 21:07:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
            "[I 2025-10-20 21:07:20,222] Trial 14 finished with value: 0.86 and parameters: {'n_estimators': 254, 'learning_rate': 0.08895346347007473, 'max_depth': 9, 'subsample': 0.6677321109201662, 'colsample_bytree': 0.8933364181786241}. Best is trial 11 with value: 0.9019607843137255.\n",
            "C:\\Users\\JaviZ\\AppData\\Local\\Temp\\ipykernel_12744\\3510787638.py:72: ExperimentalWarning: optuna.visualization.matplotlib._optimization_history.plot_optimization_history is experimental (supported from v2.2.0). The interface can change in the future.\n",
            "  fig1 = plot_optimization_history(study); _save_matplotlib(fig1.figure, \"optuna_optimization_history.png\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Mejores params: {'n_estimators': 422, 'learning_rate': 0.09478695353462088, 'max_depth': 10, 'subsample': 0.6209686514551447, 'colsample_bytree': 0.7651064381700929} | best f1=0.90196\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\JaviZ\\AppData\\Local\\Temp\\ipykernel_12744\\3510787638.py:73: ExperimentalWarning: optuna.visualization.matplotlib._param_importances.plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
            "  fig2 = plot_param_importances(study);   _save_matplotlib(fig2.figure, \"optuna_param_importances.png\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Cargando mejor modelo con get_best_model()‚Ä¶\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "faa5d04537104963b88b9d6e228434f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d368ffd6ee54590b7116f373829c65e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "Exception",
          "evalue": "Run with UUID 36e161935d6240338b2dac85ef30f83e is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m X_tr, X_v, y_tr, y_v \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Si usas tracking server remoto, descomenta y apunta all√≠:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# mlflow.set_tracking_uri(\"http://localhost:5000\")\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[32], line 101\u001b[0m, in \u001b[0;36moptimize_model\u001b[1;34m(X_train, y_train, X_valid, y_valid, experiment_name, n_trials, random_state)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[WARN] No se pudo graficar importancias: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Serializar mejor modelo en /models\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExportar mejor modelo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_id\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m td:\n\u001b[0;32m    103\u001b[0m         mp \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(td, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\JaviZ\\MDS7202\\.venv\\lib\\site-packages\\mlflow\\tracking\\fluent.py:380\u001b[0m, in \u001b[0;36mstart_run\u001b[1;34m(run_id, experiment_id, run_name, nested, parent_run_id, tags, description, log_system_metrics)\u001b[0m\n\u001b[0;32m    378\u001b[0m experiment_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(experiment_id) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(experiment_id, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m experiment_id\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(active_run_stack) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nested:\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    381\u001b[0m         (\n\u001b[0;32m    382\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun with UUID \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is already active. To start a new run, first end the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent run with mlflow.end_run(). To start a nested \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun, call start_run with nested=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(active_run_stack[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id)\n\u001b[0;32m    386\u001b[0m     )\n\u001b[0;32m    387\u001b[0m client \u001b[38;5;241m=\u001b[39m MlflowClient()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_id:\n",
            "\u001b[1;31mException\u001b[0m: Run with UUID 36e161935d6240338b2dac85ef30f83e is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
          ]
        }
      ],
      "source": [
        "print(\"[MAIN] Generando datos de ejemplo (puedes reemplazar por tus splits reales)‚Ä¶\")\n",
        "X, y = make_classification(n_samples=500, n_features=20, n_informative=8,\n",
        "                            n_redundant=4, random_state=42)\n",
        "X_tr, X_v, y_tr, y_v = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Si usas tracking server remoto, descomenta y apunta all√≠:\n",
        "# mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
        "\n",
        "optimize_model(X_tr, y_tr, X_v, y_v, experiment_name=None, n_trials=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL2iG18289j9"
      },
      "source": [
        "# **2. FastAPI (2.0 puntos)**\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://media3.giphy.com/media/YQitE4YNQNahy/giphy-downsized-large.gif\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "Con el modelo ya entrenado, la idea de esta secci√≥n es generar una API REST a la cual se le pueda hacer *requests* para as√≠ interactuar con su modelo. En particular, se le pide:\n",
        "\n",
        "- Guardar el c√≥digo de esta secci√≥n en el archivo `main.py`. Note que ejecutar `python main.py` deber√≠a levantar el servidor en el puerto por defecto.\n",
        "- Defina `GET` con ruta tipo *home* que describa brevemente su modelo, el problema que intenta resolver, su entrada y salida.\n",
        "- Defina un `POST` a la ruta `/potabilidad/` donde utilice su mejor optimizado para predecir si una medici√≥n de agua es o no potable. Por ejemplo, una llamada de esta ruta con un *body*:\n",
        "\n",
        "```json\n",
        "{\n",
        "   \"ph\":10.316400384553162,\n",
        "   \"Hardness\":217.2668424334475,\n",
        "   \"Solids\":10676.508475429378,\n",
        "   \"Chloramines\":3.445514571005745,\n",
        "   \"Sulfate\":397.7549459751925,\n",
        "   \"Conductivity\":492.20647361771086,\n",
        "   \"Organic_carbon\":12.812732207582542,\n",
        "   \"Trihalomethanes\":72.28192021570328,\n",
        "   \"Turbidity\":3.4073494284238364\n",
        "}\n",
        "```\n",
        "\n",
        "Su servidor deber√≠a retornar una respuesta HTML con c√≥digo 200 con:\n",
        "\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"potabilidad\": 0 # respuesta puede variar seg√∫n el clasificador que entrenen\n",
        "}\n",
        "```\n",
        "\n",
        "**`HINT:` Recuerde que puede utilizar [http://localhost:8000/docs](http://localhost:8000/docs) para hacer un `POST`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSausqDJ9CQh"
      },
      "source": [
        "# **3. Docker (2 puntos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNmC483flS00"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*9rafh2W0rbRJIKJzqYc8yA.gif\" width=\"500\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niMA_qsCjqlv"
      },
      "source": [
        "Tras el √©xito de su aplicaci√≥n web para generar la salida, Smapina le solicita que genere un contenedor para poder ejecutarla en cualquier computador de la empresa de agua potable.\n",
        "\n",
        "## **3.1 Creaci√≥n de Container (1 punto)**\n",
        "\n",
        "Cree un Dockerfile que use una imagen base de Python, copie los archivos del proyecto e instale las dependencias desde un `requirements.txt`. Con esto, construya y ejecute el contenedor Docker para la API configurada anteriormente. Entregue el c√≥digo fuente (incluyendo `main.py`, `requirements.txt`, y `Dockerfile`) y la imagen Docker de la aplicaci√≥n. Para la dockerizaci√≥n, aseg√∫rese de cumplir con los siguientes puntos:\n",
        "\n",
        "1. **Generar un archivo `.dockerignore`** que ignore carpetas y archivos innecesarios dentro del contenedor.\n",
        "2. **Configurar un volumen** que permita la persistencia de los datos en una ruta local del computador.\n",
        "3. **Exponer el puerto** para acceder a la ruta de la API sin tener que entrar al contenedor directamente.\n",
        "4. **Incluir im√°genes en el notebook** que muestren la ejecuci√≥n del contenedor y los resultados obtenidos.\n",
        "5. **Revisar y comentar los recursos utilizados por el contenedor**. Analice si los contenedores son livianos en t√©rminos de recursos.\n",
        "\n",
        "## **3.2 Preguntas de Smapina (1 punto)**\n",
        "Tras haber experimentado con Docker, Smapina desea profundizar m√°s en el tema y decide realizarle las siguientes consultas:\n",
        "\n",
        "- ¬øC√≥mo se diferencia Docker de una m√°quina virtual (VM)?\n",
        "- ¬øCu√°l es la diferencia entre usar Docker y ejecutar la aplicaci√≥n directamente en el sistema local?\n",
        "- ¬øC√≥mo asegura Docker la consistencia entre diferentes entornos de desarrollo y producci√≥n?\n",
        "- ¬øC√≥mo se gestionan los vol√∫menes en Docker para la persistencia de datos?\n",
        "- ¬øQu√© son Dockerfile y docker-compose.yml, y cu√°l es su prop√≥sito?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Respuestas\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"/Users/andresignacio/Desktop/Semestre X.nosync/Lab/Laboratorios/lab8/app/im√°genes/Screenshot 2025-10-21 at 21.31.13.png\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "Explicaci√≥n: En esta captura se observa el proceso de construcci√≥n de la imagen Docker mediante el comando\n",
        "docker build -t miapp ..\n",
        "Docker utiliza el archivo Dockerfile para crear una imagen basada en python:3.11-slim, copia los archivos del proyecto, instala las dependencias desde requirements.txt y prepara el entorno de ejecuci√≥n.\n",
        "La salida muestra cada capa creada y confirma que la imagen miapp:latest fue generada correctamente, cumpliendo con el primer paso del laboratorio.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"/Users/andresignacio/Desktop/Semestre X.nosync/Lab/Laboratorios/lab8/app/im√°genes/Screenshot 2025-10-21 at 21.31.46.png\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "Explicaci√≥n: En esta imagen se muestra la ejecuci√≥n del contenedor con el comando\n",
        "docker run --rm -p 8000:80 -v \"$(pwd)/data:/data\" miapp.\n",
        "La aplicaci√≥n inicia dentro del contenedor, carga el modelo de ML desde MLflow y muestra los logs de inicializaci√≥n.\n",
        "Finalmente, se visualiza el mensaje\n",
        "Uvicorn running on http://0.0.0.0:80, indicando que el servidor FastAPI est√° activo dentro del contenedor y accesible desde el host mediante la direcci√≥n http://localhost:8000.\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"/Users/andresignacio/Desktop/Semestre X.nosync/Lab/Laboratorios/lab8/app/im√°genes/Screenshot 2025-10-21 at 21.32.14.png\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "Explicaci√≥n: Esta captura corresponde a la interfaz interactiva Swagger UI generada autom√°ticamente por FastAPI, accesible en\n",
        "http://localhost:8000/docs.\n",
        "En ella se observan los endpoints disponibles (/home y /potabilidad/), junto con los esquemas de entrada (WaterSample) y salida.\n",
        "Esta vista permite probar de forma gr√°fica la API desde el navegador, demostrando que el contenedor expone correctamente la aplicaci√≥n y el puerto configurado.\n",
        "\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"/Users/andresignacio/Desktop/Semestre X.nosync/Lab/Laboratorios/lab8/app/im√°genes/Screenshot 2025-10-21 at 21.33.11.png\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "Explicaci√≥n: En esta figura se muestra una prueba del endpoint /potabilidad/ utilizando el m√©todo POST.\n",
        "Se env√≠a un conjunto de par√°metros f√≠sico-qu√≠micos del agua en formato JSON, y la API responde con la predicci√≥n de potabilidad (0).\n",
        "Esta evidencia confirma el correcto funcionamiento del modelo dentro del contenedor y la comunicaci√≥n entre el servidor FastAPI y el motor de predicci√≥n cargado desde MLflow.\n",
        "\n",
        "\n",
        "Luego se utiliza el comando docker stats, el cual muestra los recursos utilizados por el contenedor en tiempo real.\n",
        "En este caso, el contenedor consume aproximadamente 148 MiB de memoria (3.7 %) y menos del 1 % de CPU, lo que indica que la aplicaci√≥n es liviana y eficiente en ejecuci√≥n.\n",
        "El tama√±o de la imagen (~1.44 GB) se debe principalmente a dependencias de machine learning como scikit-learn y xgboost, lo cual es esperable para este tipo de proyectos.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "3.2 Preguntas de Smapina\n",
        "\n",
        "1.  Docker no crea un sistema operativo completo como una VM, sino que usa el mismo sistema del host.\n",
        "Por eso arranca mucho m√°s r√°pido y consume menos recursos. Las VMs son m√°s pesadas porque simulan todo un sistema operativo aparte.\n",
        "\n",
        "\n",
        "2. Al usar Docker, la app corre dentro de un contenedor aislado, con sus propias dependencias y versi√≥n de Python, sin afectar al sistema.\n",
        "Si se ejecuta localmente, se pueden tener conflictos con librer√≠as o versiones distintas.\n",
        "\n",
        "\n",
        "\n",
        "3. Se puede asegurar porque se utiliza la misma imagen en ambos contextos.\n",
        "El contenedor se comporta igual en cualquier m√°quina, ya que dentro tiene el mismo sistema, librer√≠as y configuraci√≥n.\n",
        "\n",
        "\n",
        "\n",
        "4. Los vol√∫menes permiten guardar datos fuera del contenedor, en una carpeta del computador.\n",
        "As√≠, aunque se borre o reinicie el contenedor, los datos quedan guardados.\n",
        "\n",
        "\n",
        "5. \n",
        "* El **Dockerfile** define paso a paso c√≥mo crear la imagen (qu√© base usar, qu√© copiar, qu√© instalar y c√≥mo ejecutar).\n",
        "* El **docker-compose.yml** sirve para levantar varios contenedores juntos (por ejemplo, backend + base de datos) con un solo comando.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xJ_ZK1IfnZW"
      },
      "source": [
        "# Conclusi√≥n\n",
        "\n",
        "√âxito!\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://i.pinimg.com/originals/55/f5/fd/55f5fdc9455989f8caf7fca7f93bd96a.gif\" width=\"500\">\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11 (xgb)",
      "language": "python",
      "name": "xgb311"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
