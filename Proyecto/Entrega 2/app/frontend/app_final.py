import gradio as gr
import requests
from typing import List

# URL de la API de predicci√≥n (asumiendo que los contenedores est√°n en la misma red Docker)
# Si se ejecuta localmente sin Docker Compose, usar http://localhost:8000
FASTAPI_URL = "http://backend:8000/predict" 

def predict_from_api(client_id: int, product_id: int, date: str) -> str:
    """
    Funci√≥n que toma los inputs de Gradio, formatea la solicitud y llama al backend de FastAPI.
    """
    
    # 1. Preparar los datos en el formato que espera FastAPI
    input_data = {
        "instances": [
            {
                "client_id": client_id,
                "product_id": product_id,
                "date": date
                # Si tu modelo tiene m√°s features, agr√©galas aqu√≠
            }
        ]
    }

    try:
        # 2. Enviar la solicitud POST al backend
        response = requests.post(FASTAPI_URL, json=input_data)
        response.raise_for_status() # Lanza excepci√≥n si el c√≥digo de estado es un error (4xx o 5xx)
        
        # 3. Procesar la respuesta
        result = response.json()
        predictions: List[float] = result.get("predictions", [])
        
        if not predictions:
            return "‚ùå Error: El backend no devolvi√≥ ninguna predicci√≥n."

        # 4. Formatear el resultado de forma clara
        prediction_value = predictions[0]
        
        # Ejemplo de formato de salida:
        return f"‚úÖ **Predicci√≥n Exitosa**\n\nEl valor predicho para esta instancia es: **{prediction_value:.4f}**"

    except requests.exceptions.ConnectionError:
        return f"‚ùå Error de Conexi√≥n: No se pudo conectar al servidor de predicciones en {FASTAPI_URL}. Aseg√∫rate de que el backend est√© corriendo."
    except requests.exceptions.HTTPError as e:
        return f"‚ùå Error del Servidor (HTTP {response.status_code}): {response.text}"
    except Exception as e:
        return f"‚ùå Error Desconocido: {e}"

# --- Definici√≥n de la Interfaz con Gradio ---

# Explicaci√≥n de uso en Markdown
explanation_text = """
## üß† MLOps Prediction Demo
Esta interfaz le permite interactuar con el modelo de Machine Learning entrenado por nuestro pipeline de Airflow.

### üìã Instrucciones de Uso:
1. **Introduzca los valores** para las tres caracter√≠sticas del modelo.
2. Haga clic en el bot√≥n **"Obtener Predicci√≥n"**.
3. El resultado aparecer√° en el cuadro de salida.
"""

# Configuraci√≥n de los componentes de entrada (ajustar seg√∫n tu modelo)
input_components = [
    gr.Number(label="Id del cliente (Introducir un entero)", value=2),
    gr.Number(label="Id del producto (Introducir un entero)", value=2),
    gr.Textbox(label="Fecha (Formato AAAA-MM-DD)", value="2025-11-17")
]

# Creaci√≥n de la interfaz
iface = gr.Interface(
    fn=predict_from_api, 
    inputs=input_components, 
    outputs=gr.Markdown(label="Resultado de la Predicci√≥n"),
    title="Sistema de Predicci√≥n MLOps",
    description=explanation_text,
    allow_flagging="never"
)

# Esto es necesario para que Gradio funcione correctamente en un contenedor Docker
if __name__ == "__main__":
    # La interfaz Gradio se inicia en 0.0.0.0 para ser accesible externamente
    iface.launch(server_name="0.0.0.0", server_port=7860)